# BUDDY

> Graph Neural Networks for Link Prediction with Subgraph Sketching

## 解决问题
- 不能区分三角形图（形成边的强烈信号）
- 不能区分自同构节点（在图的结构中等价位置的节点会生成相同的表示）
- 显式链接表示很耗资源

## 贡献
- 子图摘要（sketching）技术
  - 通过快速的近似算法和hash算法，估算集合的大小和交集特性
  - 不必生成完整子图
- 结构特征编码
  - 距离编码
  - 双半径节点标签
- BUDDY，相比ELPH减少了资源占用
  - 预先处理节点特征和边特征
  - 将预测简化为多层感知机，预测公式为：两节点表示相乘，和边表示通过一个映射函数
  - 将复杂度从与点数有关变为与摘要大小有关

## 主要内容
文章《Graph Neural Networks for Link Prediction with Subgraph Sketching》的主要内容如下：

1. **问题背景**：
   - 图神经网络（GNNs）在链接预测（Link Prediction, LP）任务中表现通常不如简单启发式方法，这是由于其表达能力的限制，如无法计算三角形（大多数LP启发式的基础）以及不能区分自同构节点（具有相同结构角色的节点）。
   - 现有基于子图的方法在性能上取得了进展，但在计算效率上存在问题。

2. **核心贡献**：
   - **ELPH模型**：提出了一种新的全图GNN模型，名为ELPH，通过消息传递使用子图的“摘要”（sketch）来近似子图GNN（SGNN）的方法，避免显式子图构造。
   - **BUDDY模型**：为了解决ELPH在大规模数据集上的GPU内存限制，提出了BUDDY模型，通过特征预计算实现了高效的扩展性，且在性能上优于现有的SGNN模型。
   - 证明了ELPH模型的表达能力比传统消息传递GNN（MPNNs）更强，并能解决自同构节点问题。

3. **实验结果**：
   - 在多个标准数据集上，ELPH和BUDDY模型在准确性和速度上均超过现有的先进模型。
   - BUDDY由于特征预计算的方式，在大规模数据集上的推断效率显著提高。

4. **方法分析**：
   - 分析了SGNN方法中的关键组件，如子图提取、结构特征（如距离编码和双半径节点标签）和特征传播的作用。
   - 提出了通过子图摘要近似结构特征的方案，从而实现对完整图的高效表示。

---

## 子图摘要（Sketching）技术

子图摘要（Sketching）技术是**ELPH模型**的核心创新之一，通过紧凑的数据表示方法来高效提取和编码图中子结构的关键信息，而无需显式构造完整子图。这一技术的关键在于利用概率性算法（如HyperLogLog和MinHash），有效地估算节点邻域的特征，解决传统图神经网络中计算效率和表达能力的不足。

---

### **1. 子图摘要技术的背景与动机**

在链接预测任务中，传统基于子图的方法（如SEAL）通过显式构造子图，提取结构特征以辅助预测。然而，这种方法存在显著缺点：
- **计算代价高**：每次训练或推断需要为目标边构造一个局部子图，其复杂度随图规模指数级增长。
- **存储开销大**：子图提取产生的大量冗余数据，难以在内存中高效存储和操作。
  
子图摘要技术通过以下方式克服这些问题：
- 通过简化计算，用**小型摘要（sketch）**代替显式的子图构造。
- 用近似统计方法有效编码子图的结构特征（如节点邻域大小、公共邻居数等）。

---

### **2. 子图摘要的主要技术**

ELPH采用了两种主要的摘要技术：**HyperLogLog（HLL）**和**MinHash**，分别用于估算集合的大小和交集特性。这两种技术通过紧凑的表示实现了高效计算。

#### **(1) HyperLogLog (HLL)**
HLL是一种近似算法，用于快速估算大规模集合的基数（即集合中唯一元素的数量）。
- **应用背景**：
  - 在图中，HLL用于估算节点的邻域大小（例如，某节点的1跳邻域、2跳邻域等的节点数）。
- **技术原理**：
  - 使用哈希函数将集合中的元素映射到一个固定大小的位数组。
  - 根据位数组中最低位连续为0的数量估算集合的基数（依据概率统计理论）。
- **在ELPH中的用途**：
  - 表示节点的\(k\)-跳邻域大小，用于生成结构特征\(Buv[d]\)（例如，从节点\(u\)到节点\(v\)的\(k\)跳范围内的节点数）。

#### **(2) MinHash**
MinHash是一种算法，用于估算两个集合之间的Jaccard相似度（交集与并集的比值）。
- **应用背景**：
  - 在图中，用于估算两个节点的邻域之间的交集大小（如公共邻居数量）。
- **技术原理**：
  - 通过对集合元素应用多个哈希函数，每次取最小的哈希值，生成固定大小的“签名”。
  - 通过对比两个集合的签名相似性（哈希值匹配的比例），估算Jaccard相似度。
- **在ELPH中的用途**：
  - 估算节点对的公共邻居数量或邻域交集大小，用于生成结构特征\(Auv[du, dv]\)（例如，节点\(u\)和节点\(v\)的\(d_u\)跳和\(d_v\)跳邻域的交集大小）。

#### **(3) HLL与MinHash的结合**
- ELPH将HLL用于估算邻域的基数，MinHash用于估算邻域的交集。
- 两者结合可以高效近似以下关键量：
  - \(|N_{du}(u)|\)：节点\(u\)的\(d_u\)跳邻域大小。
  - \(|N_{du}(u) \cap N_{dv}(v)|\)：节点\(u\)和节点\(v\)的交集邻域大小。

通过这两种技术，模型不需要显式地存储整个子图，而是通过摘要生成紧凑的结构特征表示。

---

### **3. 子图摘要的特征计算**

子图摘要用于提取和编码链接预测任务中两个节点间的重要结构信息，主要包括以下特征：

#### **(1) 结构特征\(A_{uv}[d_u, d_v]\)**
- 描述节点\(u\)和\(v\)的邻域间的交集大小：
  \[
  A_{uv}[d_u, d_v] = |N_{d_u}(u) \cap N_{d_v}(v)|
  \]
  这里，\(N_{d_u}(u)\)表示节点\(u\)的\(d_u\)跳邻域。

- **实现**：
  - 通过MinHash估算Jaccard相似度：
    \[
    J(N_{d_u}(u), N_{d_v}(v)) = \frac{|N_{d_u}(u) \cap N_{d_v}(v)|}{|N_{d_u}(u) \cup N_{d_v}(v)|}
    \]
  - 联合使用HLL计算邻域并集的大小，从而估算交集：
    \[
    |N_{d_u}(u) \cap N_{d_v}(v)| = J(N_{d_u}(u), N_{d_v}(v)) \cdot |N_{d_u}(u) \cup N_{d_v}(v)|
    \]

#### **(2) 结构特征\(B_{uv}[d]\)**
- 描述从节点\(u\)到节点\(v\)的某一跳距离范围内的节点数：
  \[
  B_{uv}[d] = |N_d(u)| - \text{已知交集特征}
  \]
- **实现**：
  - 利用HLL计算节点\(u\)的\(d\)-跳邻域的大小。

---

### **4. 子图摘要的优点**

#### **(1) 计算效率高**
- 子图摘要使用固定大小的数据结构，无论图的规模多大，计算成本主要取决于哈希操作的复杂度，与传统子图提取的指数复杂度相比显著降低。

#### **(2) 空间使用紧凑**
- 摘要技术通过压缩表示节点邻域特征，避免显式存储大规模邻域信息，显著降低内存使用。

#### **(3) 可扩展性强**
- 子图摘要技术与GNN消息传递机制自然结合，可在不增加计算成本的情况下扩展到大规模图。

---

### **5. 总结**
子图摘要（Sketching）技术是ELPH模型中关键的创新，通过引入HyperLogLog和MinHash两种紧凑表示方法，模型能够高效地捕捉和编码节点邻域的结构特征。这种技术避免了传统子图方法的高昂计算代价，同时增强了模型对三角形计数、自同构节点等问题的表达能力，是ELPH实现高效链接预测的重要基础。

---

## ELPH模型

**ELPH模型（Efficient Link Prediction with Hashing）**是文章提出的一种创新性图神经网络（GNN）方法，旨在解决传统GNN在链接预测任务中的表达能力和计算效率问题。它通过利用子图摘要（subgraph sketching）技术，避免了显式子图构造，提升了模型的可扩展性和预测能力。

---

### **1. 设计背景与动机**

传统的基于子图的GNN（如SEAL）在链接预测中通过提取子图并对其进行分析来学习边的概率，但面临以下问题：
- **高昂的计算开销**：显式构造子图并对其进行训练和推断需要耗费大量资源。
- **三角形计数与自同构问题**：传统GNN无法有效捕捉关键的结构信息（如三角形）且难以区分等价的节点对。

为了解决这些问题，ELPH提出了一种新的方法：
- **避免显式子图构造**：通过子图摘要（sketching）技术，使用紧凑表示代替完整的子图。
- **增强表达能力**：结合子图摘要的结构特征，使得模型能够捕捉到如三角形计数等关键信息。

---

### **2. ELPH的核心原理**

ELPH基于子图摘要（subgraph sketching）技术，通过高效的邻域特征计算，解决了传统GNN的表达能力限制和计算瓶颈。

#### **(1) 子图摘要（Sketching）技术**
子图摘要是一种用紧凑结构（如哈希）表示节点邻域的技术：
- **HyperLogLog (HLL)** 和 **MinHash**：
  - HLL用于估计集合的大小（如节点邻域的规模）。
  - MinHash用于估计集合间的相似性（如两个节点邻域的交集占比）。
- 使用这两种技术，可以近似计算子图中的重要结构特征，如：
  - 节点的\(k\)-跳邻域大小。
  - 两个节点的公共邻域交集大小。

#### **(2) 消息传递与结构特征编码**
在ELPH中，消息传递不仅限于节点特征，还包括通过子图摘要提取的**结构特征**。具体包括：
- **距离编码（Distance Encoding, DE）**：
  - 使用节点间的最短路径距离来生成子图的结构特征（如公共邻居数量）。
- **双半径节点标签（Double Radius Node Labeling, DRNL）**：
  - 为每个节点对分配一个独特的标签，用于标记其在子图中的位置关系。
  
这些特征被用于生成节点间的边特征\(e_{uv}\)，作为消息传递的重要组成部分。

#### **(3) 全图消息传递机制**
ELPH将子图信息直接嵌入到全图的消息传递过程中，无需显式构造子图：
- 每个节点的特征通过多层消息传递更新：
  \[
  x_u^{(l)} = \gamma^{(l)} \left( x_u^{(l-1)}, \bigoplus_{v \in N(u)} \phi^{(l)} \left( x_u^{(l-1)}, x_v^{(l-1)}, e_{uv} \right) \right)
  \]
  - \(x_u^{(l)}\)：节点\(u\)在第\(l\)层的特征。
  - \(e_{uv}\)：边特征，包含子图摘要中提取的结构信息。
  - \(\phi\)、\(\gamma\)：可学习的函数。
  - \(\bigoplus\)：聚合操作（如求和、最大值）。

#### **(4) 链接预测的边级读出（Edge-Level Readout）**
模型通过组合两个节点的特征及其边特征来计算连接概率：
\[
p(u, v) = \psi \left( x_u^{(k)} \odot x_v^{(k)}, \{e_{uv}\} \right)
\]
- 这里\(\odot\)表示逐元素乘法，\(\psi\)为一个多层感知机（MLP）。

#### **(5) 表达能力**
ELPH通过上述机制解决了传统GNN的两大问题：
- **三角形信息捕获**：通过子图摘要技术，模型能够高效估算三角形计数等特征。
- **自同构节点问题**：结合子图的结构特征（如双半径标签），模型能够区分等价节点对，从而提高区分能力。

---

### **3. 计算效率与复杂度**
相比于基于子图的方法（如SEAL），ELPH的复杂度显著降低：
- **时间复杂度**：
  - 子图摘要的计算复杂度与节点邻域大小有关，而不是整个图的规模。
  - 消息传递的复杂度与标准GNN类似（线性于边的数量）。
- **空间复杂度**：
  - 由于避免了显式的子图存储，ELPH的内存使用更为高效。

---

### **4. ELPH的优势**
1. **高效性**：
   - 通过子图摘要避免了传统子图方法中高昂的计算和存储开销。
   - 实现了对大规模图数据的快速处理。

2. **强表达能力**：
   - 能够捕捉三角形计数等复杂的图结构信息。
   - 克服了自同构节点的表达局限。

3. **实验验证**：
   - 在多个链接预测基准数据集上，ELPH超越了现有的先进模型（如SEAL），在准确性和效率上均有显著提升。

---

### **5. 总结**
ELPH是一种创新的GNN模型，通过引入子图摘要技术，将子图的结构信息嵌入全图的消息传递中，避免了昂贵的子图操作。它在解决三角形计数和自同构节点问题的同时，极大地提高了计算效率，是链接预测任务中一个强大的工具。

---

## BUDDY

**BUDDY模型**是对ELPH模型的进一步优化，旨在解决ELPH在处理**大规模数据集**时面临的内存限制和计算瓶颈。BUDDY通过**特征预计算**和高效的边特征存储与使用，大幅提高了模型的可扩展性，同时保持了与ELPH类似的预测性能。

---

### **1. BUDDY模型的设计背景**

尽管ELPH模型在计算效率和表达能力上已经优于传统子图GNN（如SEAL），但其仍然受到以下限制：
- **内存使用问题**：ELPH需要将整个图数据加载到GPU内存中，以进行消息传递操作。这在处理超大规模图（如社交网络或知识图谱）时变得不可行。
- **批处理冗余**：当图被分成小批次（batches）时，不同批次可能会重复计算大量公共邻域信息，导致效率低下。

BUDDY通过以下核心设计解决这些问题：
- **特征预计算**：提前计算节点和边的结构特征，使得后续的模型推断不再依赖完整的消息传递过程。
- **边特征缓存**：对边的关键特征进行缓存，避免重复计算。
- **独立于子图的边级推断**：通过移除子图生成的依赖，实现高效的边级操作。

---

### **2. BUDDY模型的核心原理与设计思路**

BUDDY的核心改进在于将部分计算移至**预处理阶段**，从而减轻训练和推断阶段的计算压力。以下是其关键设计：

---

#### **(1) 特征预计算（Preprocessing）**
BUDDY在模型训练前对图数据进行一次性预处理，预先计算出节点和边的结构特征。这种方式有效减少了训练和推断时的计算开销。

##### **预计算的内容**
1. **节点特征传播**：
   - 通过固定的邻域特征传播公式（类似GNN的消息传递）计算每个节点的扩展特征：
     \[
     X^{(l)}_u = \text{scatter\_mean}(X^{(l-1)}, G)
     \]
     这里，\(\text{scatter\_mean}\)表示通过稀疏矩阵计算节点的邻域平均特征。
   - 最终的节点特征是不同传播层结果的串联：
     \[
     Z_u = [X_u^{(0)} \| X_u^{(1)} \| \cdots \| X_u^{(k)}]
     \]
     - 这种方法预先完成了GNN的消息传递操作，从而在后续训练中移除了对GNN传播的依赖。

2. **边特征计算**：
   - 利用子图摘要技术（HyperLogLog和MinHash）预先计算边的结构特征，如：
     - 边的公共邻域交集大小（\(A_{uv}[d_u, d_v]\)）。
     - 节点对的距离相关特征（\(B_{uv}[d]\)）。
   - 这些特征在预处理阶段一次性计算并存储，供后续训练和推断直接使用。

---

#### **(2) 边级预测模型（Edge-Level Prediction）**
预处理完成后，BUDDY将训练和推断简化为一个基于多层感知机（MLP）的操作：
- **模型输入**：
  - 每条边的输入特征包括：
    1. 两端节点的预计算特征（\(Z_u, Z_v\)）。
    2. 预计算的边特征（\(\{B_{uv}[d], A_{uv}[d_u, d_v]\}\)）。
- **预测公式**：
  \[
  p(u, v) = \psi \left( Z_u \odot Z_v, \{B_{uv}[d], A_{uv}[d_u, d_v]\} \right)
  \]
  - 其中\(\psi\)是一个MLP，\(\odot\)为逐元素乘法。

---

#### **(3) 无需全图批处理**
通过预计算特征，BUDDY在训练和推断中：
- 不需要将整个图加载到GPU中。
- 不需要动态生成子图，从而避免了批处理中的高冗余。

---

### **3. BUDDY与ELPH的对比**

| **特性**     | **ELPH**                      | **BUDDY**                                |
| ------------ | ----------------------------- | ---------------------------------------- |
| **计算模式** | 动态计算邻域和边特征。        | 特征预计算，训练和推断无需动态操作。     |
| **内存需求** | 需要完整图数据加载到GPU内存。 | 仅需存储预计算的节点和边特征，节省内存。 |
| **效率**     | 较快，但受GPU内存限制。       | 训练和推断大幅加速，尤其适用于大图数据。 |
| **模型结构** | 使用消息传递的全图GNN。       | 简化为基于MLP的边级操作。                |
| **扩展性**   | 数据集规模受GPU限制。         | 可以轻松扩展到超大规模图。               |

#### **BUDDY的优势**
1. **更高的扩展性**：
   - 由于特征预计算模式，BUDDY能够处理规模远超GPU内存容量的图数据。
2. **更高的效率**：
   - 在大型数据集上的训练和推断时间显著降低。例如，BUDDY在Citation数据集上的训练和推断时间比SEAL快200–1000倍。
3. **性能接近ELPH**：
   - 尽管不进行动态特征传播，BUDDY的预测性能仍然与ELPH非常接近，在一些数据集上甚至超过ELPH。

---

### **4. BUDDY的时间复杂度**

BUDDY的时间复杂度主要分为两个阶段：
1. **预处理阶段**：
   - 节点特征传播和边特征计算的复杂度为：
     \[
     O(k \cdot |E| \cdot d + k \cdot |E| \cdot h)
     \]
     - \(k\)：邻域扩展的跳数。
     - \(|E|\)：边的数量。
     - \(d\)：节点特征维度。
     - \(h\)：摘要参数的大小（如HLL和MinHash的精度）。
2. **训练和推断阶段**：
   - 每条边的计算成本为：
     \[
     O(k^2 \cdot h + k \cdot d^2)
     \]
     - 与图的规模无关，仅与特征维度和摘要参数有关。

这种复杂度相比动态子图方法（如SEAL）的\(O(|E| \cdot d^2)\)显著降低。

---

### **5. 总结**

BUDDY通过**特征预计算**的设计，彻底解决了ELPH在大规模图数据上的扩展性问题，同时保持了高水平的预测性能。其优势在于：
- **高效性**：显著减少了训练和推断的时间与内存需求。
- **扩展性**：能够处理大规模图而无需分批加载或动态构造子图。
- **简单性**：通过特征预计算和边级操作，简化了模型结构。

BUDDY适合于工业场景中的超大规模图数据处理，如推荐系统和知识图谱的链接预测任务，是ELPH模型的重要延伸和优化版本。