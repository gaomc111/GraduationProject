(GCN_) (base) gaomingchen@abc:~/GCN-PyTorch$ python train.py
Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
train.py:43: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1695392036766/work/torch/csrc/utils/tensor_new.cpp:605.)
  feature = torch.sparse.FloatTensor(i.t(), v, features[2]).to(device)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.2500, 0.2500, 0.2236,  ..., 0.2000, 0.2000, 0.2000]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
/home/gaomingchen/GCN-PyTorch/utils.py:45: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392036766/work/aten/src/ATen/native/IndexingUtils.h:27.)
  i = i[:, dropout_mask]
/home/gaomingchen/GCN-PyTorch/utils.py:46: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392036766/work/aten/src/ATen/native/IndexingUtils.h:27.)
  v = v[dropout_mask]
0 13.535268783569336 0.09285714477300644
10 11.825864791870117 0.1428571343421936
20 10.288409233093262 0.2142857015132904
30 8.996515274047852 0.30000001192092896
40 7.9040727615356445 0.3499999940395355
50 6.9491143226623535 0.38571426272392273
60 6.148775100708008 0.44285711646080017
70 5.367527961730957 0.5642856955528259
80 4.837517261505127 0.5857142806053162
90 4.337121963500977 0.5714285373687744
100 3.898013114929199 0.6499999761581421
110 3.48144268989563 0.6571428179740906
120 3.102292537689209 0.6500000357627869
130 2.825829029083252 0.6857143044471741
140 2.570023536682129 0.7428570985794067
150 2.2750906944274902 0.7642857432365417
160 2.054438829421997 0.8214285969734192
170 1.9246573448181152 0.8214285969734192
180 1.7076648473739624 0.8928570747375488
190 1.54842209815979 0.9071428179740906
200 1.4315884113311768 0.8785713911056519
210 1.3597626686096191 0.9214285612106323
220 1.2655243873596191 0.9428570866584778
230 1.2521535158157349 0.8999999761581421
240 1.1154719591140747 0.9428570866584778
250 1.071033239364624 0.9142857193946838
260 0.9851671457290649 0.928571343421936
270 0.8954842686653137 0.9428570866584778
280 0.8819681406021118 0.9428572058677673
290 0.8735547661781311 0.9357142448425293
300 0.8425157070159912 0.9428572058677673
310 0.7934194803237915 0.928571343421936
320 0.7793583869934082 0.9428570866584778
330 0.770574688911438 0.9428572058677673
340 0.7078503370285034 0.964285671710968
350 0.7142202854156494 0.9499999284744263
360 0.6740667819976807 0.9428570866584778
370 0.6497426629066467 0.964285671710968
380 0.6644783020019531 0.9714285135269165
390 0.6593374013900757 0.9571428298950195
test: 0.8199999928474426