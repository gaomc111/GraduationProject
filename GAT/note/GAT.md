# GAT

> 作者：高铭辰
> 创建时间：2024/11/20
> 最后修改时间：2024/11/20

> 论文题目：GRAPH ATTENTION NETWORKS
> 代码地址(TensorFlow)：<https://github.com/PetarV-/GAT>
> Pytorch实现：<https://github.com/gordicaleksa/pytorch-GAT>

---

## 论文

**GAT结构**：堆叠图注意力层

**图注意力层（Graph Attentional Layer）**
1. 将节点的特征经过线性层变换
2. 对每一个点应用如下步骤
   1. 将其邻域点与其比较特征的相似度（相乘然后经过softmax）；可学习参数a控制其在softmax求和时的权重
   2. 按照上述softmax概率对每个邻域点的特征加权
   3. 将其求和加入这个点的特征

---

### GAT模型的结构和原理

图注意力网络（Graph Attention Networks, GAT）的结构基于图神经网络的基础框架，同时引入了自注意力机制，使其更高效且灵活地处理图结构数据。

---

#### **1. GAT的核心组件：图注意力层（Graph Attentional Layer）**

图注意力层是GAT的基本构建模块，其功能是更新每个节点的特征表示，具体包括以下步骤：

1. **输入特征**：
   - 每个节点的初始特征表示为 \( h = \{\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_N\} \)，其中 \( \mathbf{h}_i \in \mathbb{R}^F \) 表示第 \( i \) 个节点的 \( F \) 维特征。

2. **特征变换**：
   - 通过一个可学习的线性变换矩阵 \( W \in \mathbb{R}^{F' \times F} \) 将输入特征投影到新的特征空间：
     \[
     \mathbf{h}_i' = W \mathbf{h}_i
     \]
     这样可增加模型的表达能力。

3. **注意力系数计算**：
   - 自注意力机制计算节点 \( i \) 对其邻居 \( j \) 的注意力权重 \( \alpha_{ij} \)，公式为：
     \[
     e_{ij} = \text{LeakyReLU}(\mathbf{a}^T [W\mathbf{h}_i \| W\mathbf{h}_j])
     \]
     其中 \( \mathbf{a} \in \mathbb{R}^{2F'} \) 是可学习的权重向量，\(\| \) 表示特征拼接。

   - 使用 **softmax** 对邻域内的权重进行归一化，使权重具有可比性：
     \[
     \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
     \]

4. **特征聚合**：
   - 对邻域内的节点特征进行加权求和，得到节点 \( i \) 的更新特征：
     \[
     \mathbf{h}_i' = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij} W\mathbf{h}_j\right)
     \]
     其中 \( \sigma \) 是非线性激活函数（如 ReLU）。

---

#### **2. 多头注意力机制**

为了提高模型的稳定性和表达能力，GAT引入了 **多头注意力机制**：

1. 多个独立的注意力头分别计算节点特征更新：
   \[
   \mathbf{h}_i'^{(k)} = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(k)} W^{(k)} \mathbf{h}_j\right)
   \]

2. 多头注意力的结果可以通过以下两种方式合并：
   - **拼接（Concatenation）**：
     将各头输出特征拼接起来，用于中间层：
     \[
     \mathbf{h}_i' = \|_{k=1}^K \mathbf{h}_i'^{(k)}
     \]
   - **平均（Averaging）**：
     在最后一层对结果进行平均，用于分类层：
     \[
     \mathbf{h}_i' = \frac{1}{K} \sum_{k=1}^K \mathbf{h}_i'^{(k)}
     \]

---

#### **3. 模型架构**

- **输入层**：
  图的节点特征矩阵和邻接矩阵。
  
- **中间层**：
  一个或多个图注意力层，逐步更新每个节点的特征表示。

- **输出层**：
  根据任务（如分类或回归），将最终的节点特征表示输入到一个激活函数中，如 softmax（用于分类）。

---

#### **4. GAT的优点**

1. **灵活性**：
   - 自注意力机制允许动态分配邻居节点的重要性权重，无需预定义图结构特性。

2. **效率**：
   - 无需计算图拉普拉斯矩阵的特征值分解，计算复杂度低，支持大规模图数据。

3. **通用性**：
   - 适用于转导学习和归纳学习，可处理测试时完全未知的图。

4. **并行性**：
   - 计算过程可并行化，适合使用 GPU 加速。

---

GAT 模型通过结合图神经网络和注意力机制，在多种图数据集上的任务（如节点分类）中取得了最先进的性能，同时展示了强大的泛化能力和高效的学习能力。